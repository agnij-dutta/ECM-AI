{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":396802,"sourceType":"datasetVersion","datasetId":175990}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import losses\nfrom tensorflow import keras\n\nfrom tensorflow.keras.preprocessing import image\n\nfrom tensorflow.keras.layers import TextVectorization, StringLookup\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nimport cv2\nimport os\n\nimport matplotlib.image as mpimg","metadata":{"_uuid":"2898a4d2-f1b5-4aaf-83f5-79cbfb90105b","_cell_guid":"bcfb5e32-016f-49cf-ac32-d99049536791","execution":{"iopub.status.busy":"2024-09-06T04:11:31.398457Z","iopub.execute_input":"2024-09-06T04:11:31.399310Z","iopub.status.idle":"2024-09-06T04:11:48.542582Z","shell.execute_reply.started":"2024-09-06T04:11:31.399259Z","shell.execute_reply":"2024-09-06T04:11:48.541024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def group_color(styles):\n    styles[\"colorgroup\"] = -1\n    styles.loc[(styles.baseColour=='Red')|\n           (styles.baseColour=='Brown')|\n           (styles.baseColour=='Coffee Brown')|\n           (styles.baseColour=='Maroon')|\n           (styles.baseColour=='Rust')|\n           (styles.baseColour=='Burgundy')|\n           (styles.baseColour=='Mushroom Brown'),\"colorgroup\"] = 0\n    styles.loc[(styles.baseColour=='Copper'),\"colorgroup\"] = 1\n    styles.loc[(styles.baseColour=='Orange')|\n               (styles.baseColour=='Bronze')|\n               (styles.baseColour=='Skin')|\n               (styles.baseColour=='Nude'),\"colorgroup\"] = 2\n    styles.loc[(styles.baseColour=='Gold')|\n               (styles.baseColour=='Khaki')|\n               (styles.baseColour=='Beige')|\n               (styles.baseColour=='Mustard')|\n               (styles.baseColour=='Tan')|\n               (styles.baseColour=='Metallic'),\"colorgroup\"]= 3\n    styles.loc[(styles.baseColour=='Yellow'),\"colorgroup\"] = 4\n    styles.loc[(styles.baseColour=='Lime Green'),\"colorgroup\"]= 5\n    styles.loc[(styles.baseColour=='Green')|\n           (styles.baseColour=='Sea Green')|\n           (styles.baseColour=='Fluorescent Green')|\n           (styles.baseColour=='Olive'),\"colorgroup\"] = 6\n    styles.loc[(styles.baseColour=='Teal')|\n           (styles.baseColour=='Turquoise Blue'),\"colorgroup\"] = 7\n    styles.loc[(styles.baseColour=='Blue'),\"colorgroup\"]= 8\n    styles.loc[(styles.baseColour=='Navy Blue'),\"colorgroup\"] = 9\n    styles.loc[(styles.baseColour=='Purple')|\n           (styles.baseColour=='Lavender'),\"colorgroup\"] = 10\n    styles.loc[(styles.baseColour=='Pink')|\n           (styles.baseColour=='Magenta')|\n           (styles.baseColour=='Peach')|\n           (styles.baseColour=='Rose')|\n           (styles.baseColour=='Mauve'),\"colorgroup\"] = 11\n    styles.loc[(styles.baseColour=='Black')|\n           (styles.baseColour=='Charcoal'),\"colorgroup\"] = 12\n    styles.loc[(styles.baseColour=='White')|\n           (styles.baseColour=='Off White')|\n           (styles.baseColour=='Cream'),\"colorgroup\"] = 13\n    styles.loc[(styles.baseColour=='Grey')|\n           (styles.baseColour=='Silver')|\n           (styles.baseColour=='Taupe')|\n           (styles.baseColour=='Grey Melange'),\"colorgroup\"] = 14\n    styles.loc[(styles.baseColour=='Multi'),\"colorgroup\"] = 15","metadata":{"_uuid":"860fe46d-92cd-496b-8060-debc60021054","_cell_guid":"a37a135c-05c3-47ff-bfab-7a35ff23b51c","execution":{"iopub.status.busy":"2024-09-06T04:11:48.544751Z","iopub.execute_input":"2024-09-06T04:11:48.545457Z","iopub.status.idle":"2024-09-06T04:11:48.564246Z","shell.execute_reply.started":"2024-09-06T04:11:48.545411Z","shell.execute_reply":"2024-09-06T04:11:48.562739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def df_drop(styles, col, item):\n    for i in item:\n        styles = styles.drop(styles[styles[col] == i].index)\n    return styles","metadata":{"_uuid":"e685672f-a803-4840-9650-81971deaf80d","_cell_guid":"9b1915a2-2e7c-4c35-8179-0cd04f41949c","execution":{"iopub.status.busy":"2024-09-06T04:11:48.565916Z","iopub.execute_input":"2024-09-06T04:11:48.566410Z","iopub.status.idle":"2024-09-06T04:11:48.656809Z","shell.execute_reply.started":"2024-09-06T04:11:48.566354Z","shell.execute_reply":"2024-09-06T04:11:48.655599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_df():\n    \n    styles = pd.read_csv(\"/kaggle/input/fashion-product-images-small/styles.csv\", on_bad_lines='skip')\n    styles = styles.drop([\"productDisplayName\"],axis = 1) #drop useless column, we do not need name to do recommendation\n    styles = styles.drop([\"year\"],axis = 1) #drop useless column, we do not need year to do recommendation\n    styles = styles[(styles.masterCategory=='Apparel')| (styles.masterCategory=='Footwear')] # drop useless rows, we are not recommend acessories\n    styles = styles.drop(styles[styles[\"subCategory\"] == \"Innerwear\"].index) # drop useless row, we are not recommend innerwears, only outfits.\n    styles = styles.dropna() # drop NA\n    styles = df_drop(styles,\"subCategory\", [\"Apparel Set\", \"Dress\",\"Loungewear and Nightwear\",\"Saree\",\"Socks\"]) # we only recommend outfits.\n    styles[\"subCategory\"] = styles[\"subCategory\"].transform(lambda x: \"Footwear\" if(x in [\"Shoes\",\"Flip Flops\",\"Sandal\"]) else x) # Group them into one category.\n    styles = styles.drop(labels=[6695,16194,32309,36381,40000], axis=0) # drop incomplete rows\n    group_color(styles) # group the color in to color-wheel\n    return styles","metadata":{"_uuid":"fdd0a175-7a2e-4fca-8c95-15d64ec5a267","_cell_guid":"9f9c70a0-0f79-4c5a-a74c-610243eb07a1","execution":{"iopub.status.busy":"2024-09-06T04:11:48.659757Z","iopub.execute_input":"2024-09-06T04:11:48.660298Z","iopub.status.idle":"2024-09-06T04:11:48.671184Z","shell.execute_reply.started":"2024-09-06T04:11:48.660251Z","shell.execute_reply":"2024-09-06T04:11:48.669947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_input_array_subcate(df):\n    train_images = np.zeros((len(df.id),80,60,3))\n    for i in range(len(df.id)):\n        \n        #try:\n        ID = df.id.iloc[i]\n        path = f\"/kaggle/input/fashion-product-images-small/images/{ID}.jpg\"  \n        img = cv2.imread(path)\n        if img.shape != (80,60,3):\n            img = image.load_img(path, target_size=(80,60,3))\n\n        #except:\n            #print(ID)\n        \n        train_images[i] = img\n    \n    data = tf.data.Dataset.from_tensor_slices(\n      (\n        {\n          \"images\" : train_images\n       },\n\n        {\n          \"subCategory\" : df[[\"subCategory\"]]\n        }\n      )\n    )\n\n    return data","metadata":{"_uuid":"7eee6cdf-198a-406f-a070-2a4268e5709a","_cell_guid":"4611dbb3-d322-4575-9819-c9a4ea839236","execution":{"iopub.status.busy":"2024-09-06T04:11:48.672761Z","iopub.execute_input":"2024-09-06T04:11:48.673287Z","iopub.status.idle":"2024-09-06T04:11:48.689421Z","shell.execute_reply.started":"2024-09-06T04:11:48.673247Z","shell.execute_reply":"2024-09-06T04:11:48.688197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_branch(res_input, n_out, act_type, name):\n    z = layers.Dense(512, activation=\"relu\")(res_input)\n    z = layers.Dense(256, activation='relu')(z)\n    z = layers.Dense(128, activation='relu')(z)\n    z = layers.Dense(64, activation='relu')(z)\n\n    z = layers.Dense(n_out)(z)\n    z = layers.Activation(act_type, name=name)(z)\n    return z","metadata":{"_uuid":"05add202-1fe5-43eb-b030-c32cd564bbaa","_cell_guid":"1646da2e-92ac-4101-b5ad-c449e3eedf3c","execution":{"iopub.status.busy":"2024-09-06T04:11:48.690899Z","iopub.execute_input":"2024-09-06T04:11:48.691277Z","iopub.status.idle":"2024-09-06T04:11:48.703059Z","shell.execute_reply.started":"2024-09-06T04:11:48.691239Z","shell.execute_reply":"2024-09-06T04:11:48.701888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_model(width, height):\n    res50 = keras.applications.ResNet50(weights='imagenet', include_top=False, input_shape=(80,60,3))\n    res50.trainable=False\n    inputs = keras.Input(shape=(width,height,3),name = \"images\")\n    x = res50(inputs, training=False)\n    x = layers.Conv2D(32, (2, 2), activation='relu')(x)\n    x = layers.Flatten()(x)\n    x = layers.Dense(1024, activation='relu')(x)\n    # -------------------------\n\n    sub_branch = make_branch(x, len(le.classes_), 'softmax', 'subCategory')\n\n    model = keras.Model(inputs=inputs,\n                outputs=[sub_branch]\n                       )\n    return model","metadata":{"_uuid":"56e1dd2d-3a67-49e5-ab39-ba6c01f07fa4","_cell_guid":"84c15231-2465-484f-96d0-6d0203910662","execution":{"iopub.status.busy":"2024-09-06T04:11:48.704440Z","iopub.execute_input":"2024-09-06T04:11:48.704953Z","iopub.status.idle":"2024-09-06T04:11:48.721213Z","shell.execute_reply.started":"2024-09-06T04:11:48.704902Z","shell.execute_reply":"2024-09-06T04:11:48.719709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_input_xx(x):#make_input_array_subcate(styles)\n  x_input = x\n  x_input = x_input.shuffle(buffer_size = len(x_input))\n\n  x_train_size = int(0.6*len(x_input))\n  x_val_size   = int(0.2*len(x_input))\n\n  x_train = x_input.take(x_train_size).batch(2)\n  x_val   = x_input.skip(x_train_size).take(x_val_size).batch(2)\n  x_test  = x_input.skip(x_train_size + x_val_size).batch(2)\n\n  return x_train,x_val,x_test","metadata":{"_uuid":"1ad5ceec-e916-40f6-acf6-797b93b1cdf1","_cell_guid":"b4891585-f68a-42c6-9c20-2d9b7e4c9df8","execution":{"iopub.status.busy":"2024-09-06T04:11:48.722964Z","iopub.execute_input":"2024-09-06T04:11:48.723470Z","iopub.status.idle":"2024-09-06T04:11:48.735125Z","shell.execute_reply.started":"2024-09-06T04:11:48.723418Z","shell.execute_reply":"2024-09-06T04:11:48.733919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def my_le(styles):\n  articleTypeLB = LabelEncoder()\n  genderLB = LabelEncoder()\n  baseColourLB = LabelEncoder()\n  seasonLB = LabelEncoder()\n  usageLB = LabelEncoder()\n\n  styles['articleType'] = articleTypeLB.fit_transform(styles['articleType'])\n  styles['gender'] = genderLB.fit_transform(styles['gender'])\n  styles['baseColour'] = baseColourLB.fit_transform(styles['baseColour'])\n  styles['season'] = seasonLB.fit_transform(styles['season'])\n  styles['usage'] = usageLB.fit_transform(styles['usage'])\n  return styles,articleTypeLB,genderLB,baseColourLB,seasonLB,usageLB","metadata":{"_uuid":"224df63e-92d7-460b-a7a9-f586e5402c7b","_cell_guid":"e52f8a0a-ee16-4fe9-80e2-9082ca27494e","execution":{"iopub.status.busy":"2024-09-06T04:11:48.736640Z","iopub.execute_input":"2024-09-06T04:11:48.737081Z","iopub.status.idle":"2024-09-06T04:11:48.748698Z","shell.execute_reply.started":"2024-09-06T04:11:48.737040Z","shell.execute_reply":"2024-09-06T04:11:48.747465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_234_df(x):\n    styles = pd.read_csv(\"/kaggle/input/fashion-product-images-small/styles.csv\", on_bad_lines='skip')\n    styles = styles.drop([\"productDisplayName\"],axis = 1)\n    styles = styles.drop([\"year\"],axis = 1)\n    styles = styles[(styles.masterCategory=='Apparel')| (styles.masterCategory=='Footwear')]\n    styles = styles.drop(styles[styles[\"subCategory\"] == \"Innerwear\"].index)\n    styles = styles.dropna()\n    styles = df_drop(styles,\"subCategory\", [\"Apparel Set\", \"Dress\",\"Loungewear and Nightwear\",\"Saree\",\"Socks\"])\n    styles[\"subCategory\"] = styles[\"subCategory\"].transform(lambda x: \"Footwear\" if(x in [\"Shoes\",\"Flip Flops\",\"Sandal\"]) else x)\n    styles = styles.drop(labels=[6695,16194,32309,36381,40000], axis=0)\n    styles = styles[styles.subCategory == x]\n    group_color(styles)\n    styles.baseColour=styles.colorgroup\n\n    return styles","metadata":{"_uuid":"5c2c69af-afff-4fcc-a9c1-4188e940678b","_cell_guid":"1a509d10-27ac-446d-8f53-f3fa02be8c46","execution":{"iopub.status.busy":"2024-09-06T04:11:48.753555Z","iopub.execute_input":"2024-09-06T04:11:48.754015Z","iopub.status.idle":"2024-09-06T04:11:48.765032Z","shell.execute_reply.started":"2024-09-06T04:11:48.753973Z","shell.execute_reply":"2024-09-06T04:11:48.763752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#","metadata":{"_uuid":"c30537a4-6f6e-4bf3-ad0e-c4980674eacb","_cell_guid":"97c1b2a9-5d0c-4b47-a70d-c7146a49db49","execution":{"iopub.status.busy":"2024-09-06T04:11:48.766421Z","iopub.execute_input":"2024-09-06T04:11:48.766995Z","iopub.status.idle":"2024-09-06T04:11:48.782244Z","shell.execute_reply.started":"2024-09-06T04:11:48.766952Z","shell.execute_reply":"2024-09-06T04:11:48.780934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_input_array_2(df):\n    valid_ids = []\n    train_images = []\n\n    for i in range(len(df.id)):\n        ID = df.id.iloc[i]\n        path = f\"/kaggle/input/fashion-product-images-small/images/{ID}.jpg\"\n        \n        # Check if file exists\n        if not os.path.exists(path):\n            print(f\"File does not exist: {path}\")\n            continue\n        \n        img = cv2.imread(path)\n        \n        # Check if image is loaded successfully\n        if img is None:\n            print(f\"Warning: Could not load image {ID} at {path}\")\n            continue\n        \n        # Resize if necessary\n        if img.shape != (80, 60, 3):\n            img = image.load_img(path, target_size=(80, 60, 3))\n            img = image.img_to_array(img)\n        \n        train_images.append(img / 255.0)\n        valid_ids.append(ID)\n    \n    # Update the DataFrame to include only valid IDs\n    df_valid = df[df.id.isin(valid_ids)]\n    \n    # Convert images and corresponding data to a TensorFlow dataset\n    data = tf.data.Dataset.from_tensor_slices(\n        (\n            {\"images\": np.array(train_images)},\n            {\n                \"articleType\": df_valid[[\"articleType\"]].values,\n                \"gender\": df_valid[[\"gender\"]].values,\n                \"baseColour\": df_valid[[\"baseColour\"]].values,\n                \"season\": df_valid[[\"season\"]].values,\n                \"usage\": df_valid[[\"usage\"]].values\n            }\n        )\n    )\n\n    return data","metadata":{"_uuid":"5c84b1cf-fae1-43ad-a52c-f402e81d60f3","_cell_guid":"1a86d4f4-5beb-474a-b154-0900df84fd61","execution":{"iopub.status.busy":"2024-09-06T04:11:48.783919Z","iopub.execute_input":"2024-09-06T04:11:48.784369Z","iopub.status.idle":"2024-09-06T04:11:48.796699Z","shell.execute_reply.started":"2024-09-06T04:11:48.784284Z","shell.execute_reply":"2024-09-06T04:11:48.795516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"styles = get_df()\nstyles[\"subCategory\"].unique() # we can check by this code that we only have three subcategory now.\n\n\"\"\"# Model-1: \"\"\"\nle = LabelEncoder()\nstyles[\"subCategory\"] = le.fit_transform(styles[\"subCategory\"])\nstyles.head()\nle.classes_\nsub_train,sub_val,sub_test = make_input_xx(make_input_array_subcate(styles))\nsub_model = build_model(80, 60)\nsub_model.summary()","metadata":{"_uuid":"0bf8328b-a094-4821-9b1f-b5cf7e47dc3e","_cell_guid":"60525ff4-2c35-40e2-b8ec-90b9ae5f67a0","execution":{"iopub.status.busy":"2024-09-06T04:11:48.798356Z","iopub.execute_input":"2024-09-06T04:11:48.798774Z","iopub.status.idle":"2024-09-06T04:15:34.938636Z","shell.execute_reply.started":"2024-09-06T04:11:48.798721Z","shell.execute_reply":"2024-09-06T04:15:34.937523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.utils import plot_model\nplot_model(sub_model)\n\nsub_model.compile(optimizer='adam',\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n\nsub_history = sub_model.fit(sub_train, \n                    epochs=5, \n                    steps_per_epoch = 2000,\n                    validation_data = sub_val)","metadata":{"_uuid":"679e749a-e950-44a1-9ad3-bfac91245067","_cell_guid":"8758f640-b91e-4d27-9988-51b24ac8d67f","execution":{"iopub.status.busy":"2024-09-06T04:15:34.940225Z","iopub.execute_input":"2024-09-06T04:15:34.940699Z","iopub.status.idle":"2024-09-06T04:31:46.369800Z","shell.execute_reply.started":"2024-09-06T04:15:34.940647Z","shell.execute_reply":"2024-09-06T04:31:46.367818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_model.save(\"model_sub.h5\")","metadata":{"_uuid":"bfa239ce-308d-4186-8c24-1fe82b28a834","_cell_guid":"480f93e6-7ea1-4247-a979-cc4d21b1b8b1","execution":{"iopub.status.busy":"2024-09-06T04:31:46.372631Z","iopub.execute_input":"2024-09-06T04:31:46.373261Z","iopub.status.idle":"2024-09-06T04:31:47.113558Z","shell.execute_reply.started":"2024-09-06T04:31:46.373198Z","shell.execute_reply":"2024-09-06T04:31:47.111930Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_model = tf.keras.models.load_model(\"model_sub.h5\")\ntest_model.evaluate(sub_test)","metadata":{"_uuid":"f12f8e24-0ea7-4bae-88cb-da0f97022847","_cell_guid":"6ae161a9-42fa-49eb-9836-602e5655f176","execution":{"iopub.status.busy":"2024-09-06T04:31:47.115171Z","iopub.execute_input":"2024-09-06T04:31:47.115534Z","iopub.status.idle":"2024-09-06T04:33:51.877246Z","shell.execute_reply.started":"2024-09-06T04:31:47.115499Z","shell.execute_reply":"2024-09-06T04:33:51.875823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_model(width, height, articleTypeLB,genderLB,baseColourLB,seasonLB,usageLB):\n    # -------------------------\n    res50 = keras.applications.ResNet50(weights='imagenet', include_top=False, input_shape=(80,60,3))\n    res50.trainable=False\n    inputs = keras.Input(shape=(width,height,3),name = \"images\")\n    x = res50(inputs, training=False)\n    \n    x = layers.Flatten()(x)\n    x = layers.Dense(1024, activation='relu')(x)\n    # -------------------------\n\n    article_branch = make_branch(x, len(articleTypeLB.classes_), 'softmax', 'articleType')\n    gender_branch = make_branch(x, len(genderLB.classes_), 'softmax', 'gender')\n    color_branch = make_branch(x, len(baseColourLB.classes_), 'softmax', 'baseColour')\n    season_branch = make_branch(x, len(seasonLB.classes_), 'softmax', 'season')\n    usage_branch = make_branch(x, len(usageLB.classes_), 'softmax', 'usage')\n\n    model = keras.Model(inputs=inputs,\n                outputs=[article_branch, gender_branch, color_branch, \n                            season_branch, usage_branch]\n                       )\n    return model","metadata":{"_uuid":"46e69b9c-027a-4819-9897-d0fa84ecaa85","_cell_guid":"c96fef86-8b07-407e-a2d4-81d521a54826","execution":{"iopub.status.busy":"2024-09-06T04:33:51.878907Z","iopub.execute_input":"2024-09-06T04:33:51.879357Z","iopub.status.idle":"2024-09-06T04:33:51.891332Z","shell.execute_reply.started":"2024-09-06T04:33:51.879317Z","shell.execute_reply":"2024-09-06T04:33:51.890095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"# Model 234\"\"\"\ntop_df = get_234_df(\"Topwear\")\nbottom_df = get_234_df(\"Bottomwear\")\nfoot_df = get_234_df(\"Footwear\")\n\ntop_df,top_art,top_gen,top_base,top_sea,top_usage = my_le(top_df)\nbottom_df,bottom_art,bottom_gen,bottom_base,bottom_sea,bottom_usage = my_le(bottom_df)\nfoot_df,foot_art,foot_gen,foot_base,foot_sea,foot_usage = my_le(foot_df)\n\nfoot_usage.classes_\n\ntop_base_model = build_model(80,60,top_art,top_gen,top_base,top_sea,top_usage)\nbottom_base_model = build_model(80,60,bottom_art,bottom_gen,bottom_base,bottom_sea,bottom_usage)\nfoot_base_model = build_model(80,60,foot_art,foot_gen,foot_base,foot_sea,foot_usage)\n\n\ntop_train, top_val, top_test = make_input_xx(make_input_array_2(top_df))\nbottom_train, bottom_val, bottom_test = make_input_xx(make_input_array_2(bottom_df))\nfoot_train, foot_val, foot_test = make_input_xx(make_input_array_2(foot_df))\n\ntop_base_model.compile(\n    optimizer='adam',\n    loss={\n        'articleType': 'sparse_categorical_crossentropy',\n        'gender': 'sparse_categorical_crossentropy',\n        'baseColour': 'sparse_categorical_crossentropy',\n        'season': 'sparse_categorical_crossentropy',\n        'usage': 'sparse_categorical_crossentropy'\n    },\n    metrics={\n        'articleType': 'accuracy',\n        'gender': 'accuracy',\n        'baseColour': 'accuracy',\n        'season': 'accuracy',\n        'usage': 'accuracy'\n    }\n)\nbottom_base_model.compile(\n    optimizer='adam',\n    loss={\n        'articleType': 'sparse_categorical_crossentropy',\n        'gender': 'sparse_categorical_crossentropy',\n        'baseColour': 'sparse_categorical_crossentropy',\n        'season': 'sparse_categorical_crossentropy',\n        'usage': 'sparse_categorical_crossentropy'\n    },\n    metrics={\n        'articleType': 'accuracy',\n        'gender': 'accuracy',\n        'baseColour': 'accuracy',\n        'season': 'accuracy',\n        'usage': 'accuracy'\n    }\n)\nfoot_base_model.compile(\n    optimizer='adam',\n    loss={\n        'articleType': 'sparse_categorical_crossentropy',\n        'gender': 'sparse_categorical_crossentropy',\n        'baseColour': 'sparse_categorical_crossentropy',\n        'season': 'sparse_categorical_crossentropy',\n        'usage': 'sparse_categorical_crossentropy'\n    },\n    metrics={\n        'articleType': 'accuracy',\n        'gender': 'accuracy',\n        'baseColour': 'accuracy',\n        'season': 'accuracy',\n        'usage': 'accuracy'\n    }\n)","metadata":{"_uuid":"0bf6b594-9c59-4d79-9151-e306348394f3","_cell_guid":"6079203e-7a68-4a4a-8e0d-e0f009c1e383","execution":{"iopub.status.busy":"2024-09-06T04:33:51.893111Z","iopub.execute_input":"2024-09-06T04:33:51.893518Z","iopub.status.idle":"2024-09-06T04:34:02.329558Z","shell.execute_reply.started":"2024-09-06T04:33:51.893478Z","shell.execute_reply":"2024-09-06T04:34:02.327728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"top_history = top_base_model.fit(top_train, \n                    epochs=10, \n                    steps_per_epoch = 500,\n                    validation_data = top_val)\n\ntop_base_model.evaluate(top_test)\ntop_base_model.save(\"model_2.1.h5\")","metadata":{"_uuid":"db04b170-4e6b-412d-990b-f8e7ec5f79e1","_cell_guid":"9cf83b67-11f1-492c-aa30-033d5304d5ad","execution":{"iopub.status.busy":"2024-09-06T04:34:02.330630Z","iopub.status.idle":"2024-09-06T04:34:02.331164Z","shell.execute_reply.started":"2024-09-06T04:34:02.330914Z","shell.execute_reply":"2024-09-06T04:34:02.330941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bottom_history = bottom_base_model.fit(bottom_train, \n                    epochs=15, \n                    steps_per_epoch = 50,\n                    validation_data = bottom_val)\n\nbottom_base_model.evaluate(bottom_test)\n\nbottom_base_model.save(\"model_2.2.h5\")","metadata":{"_uuid":"8fd1adc9-4f2f-4335-b85c-0cafeb550515","_cell_guid":"3721bc88-920a-44a9-aef1-bbaeb92b118a","execution":{"iopub.status.busy":"2024-09-06T04:34:02.332899Z","iopub.status.idle":"2024-09-06T04:34:02.333388Z","shell.execute_reply.started":"2024-09-06T04:34:02.333160Z","shell.execute_reply":"2024-09-06T04:34:02.333184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"foot_history = foot_base_model.fit(foot_train, \n                    epochs=5, \n                    steps_per_epoch = 2000,\n                    validation_data = foot_val)\n\nfoot_base_model.evaluate(foot_test)\n\nfoot_base_model.save(\"model_2.2.h5\")","metadata":{"_uuid":"4f8e2f1a-68b8-442d-a100-a8995e3abae7","_cell_guid":"d377c804-e5c4-45ce-8f8d-3f45d9a39dab","execution":{"iopub.status.busy":"2024-09-06T04:34:02.334924Z","iopub.status.idle":"2024-09-06T04:34:02.335420Z","shell.execute_reply.started":"2024-09-06T04:34:02.335190Z","shell.execute_reply":"2024-09-06T04:34:02.335212Z"},"trusted":true},"execution_count":null,"outputs":[]}]}